{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"R per l'analisi statistica multivariata\"\n",
        "subtitle: \"Unità K: analisi di verosimiglianza\"\n",
        "author: \"Tommaso Rigon\"\n",
        "institute: \"Università Milano-Bicocca\"\n",
        "editor: visual\n",
        "execute:\n",
        "  echo: true\n",
        "  warning: true\n",
        "editor_options: \n",
        "  chunk_output_type: console\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Argomenti affrontati\n",
        "\n",
        "-   Aspetti numerici legati all'inferenza tramite verosimiglianza\n",
        "-   Stima numerica di massima verosimiglianza\n",
        "-   Il caso multiparametrico (grafici `contour`)\n",
        "\n",
        "::: callout-note\n",
        "#### Nota\n",
        "\n",
        "Gli esercizi **R** associati sono disponibili a [questo link](https://tommasorigon.github.io/introR/exe/es_4.html)\n",
        ":::\n",
        "\n",
        "## Un breve sommario\n",
        "\n",
        "In questa unità procederemo tramite **esempi**, volti a mostrare alcuni **aspetti numerici** e **grafici** legati alla verosimiglianza in modelli parametrici.\n",
        "\n",
        "In primo luogo, considereremo un esempio di modello con parametro **scalare**.\n",
        "\n",
        "Quindi, considereremo un esempio modello con parametro **vettoriale**.\n",
        "\n",
        "Per ovvie ragioni, non avremo tempo / modo di ripercorrere l'intero programma di Statistica II.\n",
        "\n",
        "Pertanto è di fondamentale importanza che gli argomenti di **inferenza statistica** siano ben chiari.\n",
        "\n",
        "## Ripasso & notazione\n",
        "\n",
        "Nel caso assolutamente continuo, un **modello statistico** è una collezione di funzioni di densità $$\n",
        "\\mathcal{F} = \\{f(\\cdot ; \\theta) : \\theta \\in \\Theta\\},\n",
        "$$ indicizzata da un vettori di parametri $\\theta \\in \\Theta$, dove $\\Theta \\subseteq \\mathbb{R}^p$ è lo **spazio parametrico**.\n",
        "\n",
        "Nel **caso discreto**, la definizione di modello statistico è la medesima, ma considerando delle funzioni di probabilità $p(x ; \\theta)$ al posto delle densità.\n",
        "\n",
        "Assumiamo inoltre che che $y = (y_1,\\dots,y_n)$ sia un campione (spesso iid) con legge congiunta $f(y; \\theta)$, per un qualche **ignoto** valore del parametro $\\theta$.\n",
        "\n",
        "Relativamente ad un modello statistico $\\mathcal{F}$ di cui è stato osservato un campione $y$, si chiama **verosimiglianza** la funzione da $\\Theta$ in $\\mathbb{R} \\cup \\{0\\}$ $$\n",
        "\\mathcal{L}(\\theta) = \\mathcal{L}(\\theta; y) = C \\: f(y; \\theta),\n",
        "$$ dove $C = C(y)$ è una costante positiva che non dipende da $\\theta$.\n",
        "\n",
        "## Ripasso e notazione\n",
        "\n",
        "Assumiamo che campione $y = (y_1,\\dots,y_n)$ sia composto da realizzazioni **indipendenti** ed **identicamente distribute** da una variabile casuale legge $f(y; \\theta)$. Quindi, si ottiene $$\n",
        "    \\mathcal{L}(\\theta) = \\mathcal{L}(\\theta; y) = C \\: \\prod_{i=1}^n f(y_i; \\theta).\n",
        "    $$\n",
        "\n",
        "Dato che $\\mathcal{L}(\\theta)$ è non-negativa, spesso si lavora con la funzione di **log-verosimiglianza** $$\n",
        "    \\ell(\\theta) = \\ell(\\theta; y) = \\log\\mathcal{L}(\\theta) = c + \\sum_{i=1}^n\\log f(y_i; \\theta),\n",
        "    $$ dove $c \\in \\mathbb{R}$ è una costante additiva che non dipende da $\\theta$.\n",
        "\n",
        "## Ripasso & notazione\n",
        "\n",
        "La **stima di massima verosimiglianza** (SMV) è il valore $\\hat{\\theta}$ che rende massima la funzione di verosimiglianza $\\mathcal{L}(\\theta)$ sullo spazio $\\Theta$, cioè tale per cui $$\n",
        "\\mathcal{L}(\\hat{\\theta}) = \\sup_{\\theta \\in \\Theta}\\mathcal{L}(\\theta). \n",
        "$$\n",
        "\n",
        "La SMV rende massimo anche la funzione di log-verosimiglianza, per cui $$\n",
        "\\ell(\\hat{\\theta}) = \\sup_{\\theta \\in \\Theta}\\ell(\\theta).\n",
        "$$\n",
        "\n",
        "Di conseguenza, sotto opportune **condizioni di regolarità**, la SMV si ottiene come soluzione dell'equazione $\\ell'(\\theta) = 0$.\n",
        "\n",
        "Non è detto che la SMV esista.\n",
        "\n",
        "La SMV $\\hat{\\theta} = \\hat{\\theta}(y)$ è una **funzione del campione**, anche se a volte tale funzione non è rappresentabile esplicitamente.\n",
        "\n",
        "## Parametro scalare: modello esponenziale\n",
        "\n",
        "Sia $y = (y_1,\\dots,y_n)$ un campione iid da una variabile casuale esponenziale con **tasso di guasto** $\\lambda$, ovvero $Y \\sim \\text{Exp}(\\lambda)$.\n",
        "\n",
        "Si ricordi che la densità di un modello esponenziale è $$\n",
        "f(y; \\lambda) = \\lambda e^{-\\lambda y}, \\qquad y, \\lambda > 0.\n",
        "$$\n",
        "\n",
        "Di conseguenza, la funzione di log-verosimiglianza associata al campione $y$ è $$\n",
        "\\ell(\\lambda) = \\ell(\\lambda; y) = n \\log \\lambda - \\lambda\\sum_{i=1}^ny_i. \n",
        "$$\n",
        "\n",
        "::: callout-note\n",
        "#### Esercizio\n",
        "\n",
        "Nel caso la derivazione di $\\ell(\\lambda)$ non fosse immediatamente chiara, si svolgano per esercizio tutti i passaggi.\n",
        ":::\n",
        "\n",
        "## Parametro scalare: modello esponenziale\n",
        "\n",
        "La funzione che calcola la verosimiglianza in \\textbf{R} può essere implementata in vari modi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loglik <- function(lambda, y) {\n",
        "  length(y) * log(lambda) - lambda * sum(y)\n",
        "}\n",
        "\n",
        "loglik2 <- function(lambda, y) {\n",
        "  sum(dexp(y, rate = lambda, log = TRUE))\n",
        "}\n",
        "\n",
        "loglik3 <- function(lambda, y) {\n",
        "  sum(log(lambda) - lambda * y)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consideriamo il dataset `aircondit7` della libreria `boot`, contenente i **tempi di rottura** relativi al sistema di condizionamento di alcuni aeroplani."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(boot)\n",
        "data(\"aircondit7\")\n",
        "y <- aircondit7$hours"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "curve(loglik(x, y), 1e-4, 0.06) # log-verosimiglianza nell'intervallo (0.001, 0.06)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Le funzione *vettorizzabili* I\n",
        "\n",
        "Le tre funzioni implementate sono **apparentemente** equivalenti:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loglik(0.01, y)\n",
        "loglik2(0.01, y)\n",
        "loglik3(0.01, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tuttavia, solamente la prima è correttamente **vettorizzata**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Risultato corretto\n",
        "loglik(c(0.01, 0.02, 0.03), y)\n",
        "\n",
        "# I seguenti comandi producono invece dei risultati errati\n",
        "loglik2(c(0.01, 0.02, 0.03), y)\n",
        "loglik3(c(0.01, 0.02, 0.03), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Le funzioni *vettorizzabili* II\n",
        "\n",
        "Il linguaggio **R** ama lavorare in modo **vettoriale** (e tipicamente odia i cicli `for`...)\n",
        "\n",
        "Solamente `loglik` è una funzione **vettorizzata** rispetto al parametro `lambda`.\n",
        "\n",
        "In altri termini, la funzione `loglik` può essere valutata in un vettore di punti ed il risultato sarà il vettore di valori di log-verosimiglianza associati.\n",
        "\n",
        "Fortunatamente, le funzioni possono essere \"convertite\" come segue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loglik2 <- Vectorize(loglik2, vectorize.args = \"lambda\")\n",
        "loglik3 <- Vectorize(loglik3, vectorize.args = \"lambda\")\n",
        "\n",
        "loglik(c(0.01, 0.02, 0.03), y)\n",
        "loglik2(c(0.01, 0.02, 0.03), y)\n",
        "loglik3(c(0.01, 0.02, 0.03), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Qualche cenno alle performance\n",
        "\n",
        "Nonostante producano lo stesso risultato, le tre funzioni non hanno la stessa efficienza in termini di tempo.\n",
        "\n",
        "Una funzione **nativamente** vettorizzata è tipicamente più rapida di una conversione.\n",
        "\n",
        "Possiamo misurare le **performance** usando, ad esempio, il pacchetto `microbenchmark`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(microbenchmark) # Se assente, va installata\n",
        "microbenchmark(\n",
        "  L1 = loglik(c(0.01, 0.02, 0.03), y),\n",
        "  L2 = loglik2(c(0.01, 0.02, 0.03), y),\n",
        "  L3 = loglik3(c(0.01, 0.02, 0.03), y)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## La stima di massima verosimiglianza\n",
        "\n",
        "La **funzione punteggio** e l'**informazione osservata** del modello esponenziale sono pari a $$\n",
        "\\ell'(\\lambda) = \\frac{n}{\\lambda} - \\sum_{i=1}^ny_i, \\qquad j(\\lambda) = - \\ell''(\\lambda) = \\frac{n}{\\lambda^2}. \n",
        "$$\n",
        "\n",
        "Entrambe queste funzioni sono utili per motivi inferenziali. Ad esempio la **stima di massima verosimiglianza** si ottiene (nei casi regolari) ponendo $\\ell'(\\lambda) = 0$.\n",
        "\n",
        "In questo caso quindi avremo $$\n",
        "\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^ny_i} = \\frac{1}{\\bar{y}}. \n",
        "$$\n",
        "\n",
        "L'informazione osservata (in questo caso coincidente con l'informazione attesa) è invece estremamente utile per costruire ad esempio **intervalli di confidenza**\n",
        "\n",
        "La stima di massima verosimiglianza (numerica)\n",
        "\n",
        "Come vederemo in seguito, risolvere l'equazione $\\ell'(\\lambda) = 0$ può essere problematico (ovviamente non nel caso esponenziale...).\n",
        "\n",
        "## Procedure numeriche\n",
        "\n",
        "In questi casi \"difficili\", la stima di massima verosimiglianza può essere ottenuta tramite procedure **numeriche**.\n",
        "\n",
        "In altri termini, utilizzeremo un **algoritmo iterativo** che produce una sequenza di valori $\\lambda_1, \\lambda_2, \\dots$, tali che, quantomeno idealmente, $$\n",
        "\\ell(\\lambda_{k+1}) \\ge \\ell(\\lambda_k). \n",
        "$$\n",
        "\n",
        "Il nuovo valore $\\lambda_{k+1}$ è tipicamente ottenuto a partire dal valore precedente $\\lambda_k$.\n",
        "\n",
        "Pertanto, questa tipologia di algoritmi hanno bisogno di un **valore iniziale** $\\lambda_1$ definito dall'utente.\n",
        "\n",
        "Dopo un certo numero di iterazioni, quando non si osservano più variazioni significative in termini di $\\lambda$ e/o $\\ell(\\lambda)$, l'algoritmo si ferma e si dice che è arrivato a **convergenza**.\n",
        "\n",
        "## Il metodo di Newton-Raphson I\n",
        "\n",
        "Nel metodo di **Newton-Raphson**, consideriamo lo sviluppo di Taylor della funzione di log-verosimiglianza $\\ell(\\lambda)$ nel generico punto $\\lambda_k$, troncato al termine **quadratico** $$\n",
        "\\ell(\\lambda) \\approx \\ell(\\lambda_k) + \\ell'(\\lambda_k)(\\lambda - \\lambda_k) +  \\frac{\\ell''(\\lambda_k)}{2}(\\lambda - \\lambda_k)^2.\n",
        "$$\n",
        "\n",
        "Massimizziamo tale sviluppo, equivalente a trovare il vertice di una parabola, e quindi otteniamo l'equazione di verosimiglianza approssimata $$\n",
        "\\ell'(\\lambda) - j(\\lambda_k)(\\lambda - \\lambda_k) = 0,\n",
        "$$ che risolta rispetto a $\\lambda$ porta allo schema iterativo $$\n",
        "\\lambda_{k + 1} = \\lambda_k + j(\\lambda_k)^{-1}\\ell'(\\lambda_k), \\qquad k=1,2,\\dots.\n",
        "$$\n",
        "\n",
        "::: callout-note\n",
        "#### Nota\n",
        "\n",
        "Il metodo considera di fatto una serie di **approssimazioni paraboliche** della log-verosimiglianza, per le quali si va ogni volta a valutare il punto di massimo.\n",
        ":::\n",
        "\n",
        "## Il metodo di Newton-Raphson II\n",
        "\n",
        "Il metodo **Newton-Raphson** appena descritto si può implementare in \\textbf{R} come segue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lambda <- numeric(10); lambda[1] <- 0.005 # Punto iniziale (cosa succede cambiandolo?)\n",
        "n <- length(y); sum_y <- sum(y)\n",
        "# Algorithmo Newton-Raphson\n",
        "for(k in 1:5){\n",
        "  score <- n / lambda[k] - sum_y # Funzione score\n",
        "  obs_info <- n / lambda[k]^2 # Informazione osservata\n",
        "  lambda[k + 1] <- lambda[k] + score / obs_info # Passo iterativo\n",
        "  print(c(lambda[k + 1], loglik(lambda[k + 1], y)), digits = 3) # Mostro i risultati\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il risultato esatto, in questo caso, è facile da calcolare e lo riportiamo per un confronto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lambda_hat <- 1 / mean(y)\n",
        "c(lambda_hat, loglik(lambda_hat, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metodi di massimizzazione\n",
        "\n",
        "In pratica, in **R** esistono diverse funzioni per la massimizzazione, basate su varie elaborazioni del metodo di Newton-Raphson (che noi non vedremo).\n",
        "\n",
        "Tutte queste funzioni richiedono un **valore iniziale** ma non necessitano dei valori delle derivate, che vengono approssimate internamente.\n",
        "\n",
        "Le funzioni richiedono inoltre un qualche **criterio di arresto**.\n",
        "\n",
        "Alcuni metodi permettono l'utilizzo di **vincoli sui parametri**.\n",
        "\n",
        "::: callout-note\n",
        "#### Nota\n",
        "\n",
        "Per ragioni storiche, le funzioni di **R** identificano sempre il minimo della funzione cercata. Tuttavia: $$\n",
        "\\arg \\max_{\\lambda \\in \\mathbb{R}} \\:\\ell(\\lambda) = \\arg \\min_{\\lambda \\in \\mathbb{R}} \\:- \\ell(\\lambda).\n",
        "$$ In altri termini, è sufficiente **cambiare di segno** la funzione obiettivo.\n",
        ":::\n",
        "\n",
        "## La funzione `nlminb`\n",
        "\n",
        "La funzione `nlminb` effettua **minimizzazioni numeriche**, anche vettoriali.\n",
        "\n",
        "È una routine generalmente considerata più stabile, robusta ed affidabile della funzione concorrente `optim`, che vedremo in seguito.\n",
        "\n",
        "Inoltre, consente di incorporare dei **vincoli** sui parametri, qualora questi fossero presenti.\n",
        "\n",
        "Nel modello esponenziale, per esempio, si ha il vincolo $\\lambda > 0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# start = 1 significa che il valore iniziale è pari a 1\n",
        "# lower = 1e-5 implica che lambda > 0\n",
        "fit_exp <- nlminb(start = 1, objective = function(lambda) - loglik(lambda, y), lower = 1e-5)\n",
        "fit_exp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lambda_hat <- fit_exp$par # Stima di massima verosimiglianza\n",
        "curve(loglik(x, y), 0.001, 0.05)\n",
        "abline(v = lambda_hat, lty = \"dotted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optim(par = 1, fn = function(lambda) - loglik(lambda, y), lower = 1e-5, method = \"L-BFGS-B\", hessian = TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Informazione osservata\n",
        "obs_info <- length(y) / lambda_hat^2\n",
        "obs_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adeguatezza del modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot(ecdf(y))\n",
        "curve(pexp(x, lambda_hat), col = \"red\", add = TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parametro vettoriale: il modello Weibull I\n",
        "\n",
        "Sia $y = (y_1,\\dots,y_n)$ un campione iid da una variabile casuale Weibull di **parametri** $(\\gamma, \\beta)$, ovvero $Y \\sim \\text{Weib}(\\gamma, \\beta)$.\n",
        "\n",
        "Si ricordi che la densità di un modello Weibull è $$\n",
        "f(y; \\gamma, \\beta) = \\frac{\\gamma}{\\beta} \\left(\\frac{y}{\\beta}\\right)^{\\gamma - 1} e^{-(y / \\beta)^\\gamma}, \\qquad y, \\gamma, \\beta > 0.\n",
        "$$\n",
        "\n",
        "Di conseguenza, la funzione di **log-verosimiglianza** associata al campione $y$ è $$\n",
        "\\ell(\\gamma, \\beta) = \\ell(\\gamma, \\beta; y) = n \\log \\gamma - n \\gamma \\log \\beta + \\gamma \\sum_{i=1}^n\\log y_i - \\sum_{i=1}^n \\left(\\frac{y_i}{\\beta}\\right)^\\gamma. \n",
        "$$\n",
        "\n",
        "::: callout-note\n",
        "#### Esercizio\n",
        "\n",
        "Si ottenga la precedente log-verosimiglianza (carta e penna)\n",
        ":::\n",
        "\n",
        "## Parametro vettoriale: il modello Weibull II\n",
        "\n",
        "La funzione che calcola la verosimiglianza in **R** può essere implementata in vari modi. Ne presentiamo qui uno dei tanti basato sulla funzione `dweibull`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loglik <- function(par, y) {\n",
        "  sum(dweibull(y, shape = par[1], scale = par[2], log = TRUE))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Siamo interessati a capire se il modello Weibull, che **generalizza** quello esponenziale, rappresenta una scelta modellistica migliore.\n",
        "\n",
        "Consideriamo anche in questo caso il dataset `aircondit7` della libreria `boot`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(boot)\n",
        "data(\"aircondit7\")\n",
        "y <- aircondit7$hours"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grafico della verosimiglianza I (facoltativo)\n",
        "\n",
        "In generale non è possibile disegnare dei grafici della verosimiglianza con parametri vettoriali.\n",
        "\n",
        "L'unica eccezione è il caso $p = 2$, in cui possiamo usare i cosiddetti grafici `contour`, basati sulle **curve di livello**.\n",
        "\n",
        "Per fare questo tipo di grafico, bisogna procedere per step.\n",
        "\n",
        "-   **Step 1**. Anzitutto bisogna definire un \\empb{intervallo di punti} per ciascuna componente del parametro.\n",
        "\n",
        "-   **Step 2**. Bisogna quindi calcolare la log-verosimiglianza in ciascuna coppia di punti della griglia definita dal **prodotto cartesiano** dei due intervalli.\n",
        "\n",
        "## Grafico della verosimiglianza II (facoltativo)\n",
        "\n",
        "Gli estremi degli intervalli vanno scelti **manualmente**. In pratica, bisogna procedere per tentativi se si vuole disegnare la verosimiglianza in una regione \"sensata\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Definizione degli intervalli di punti\n",
        "gamma <- seq(0.1, 2.25, length = 200)\n",
        "beta <- seq(0.5, 400, length = 200)\n",
        "\n",
        "# Ottenimento della griglia tramite prodotto cartesiano \n",
        "parvalues <- expand.grid(gamma, beta)\n",
        "\n",
        "# Calcolo valori di verosimiglianza\n",
        "llikvalues <- apply(parvalues, 1, loglik, y = y)\n",
        "\n",
        "# Ri-organizzazione dei valori della log-verosimiglianza in forma matriciale\n",
        "llikvalues <- matrix(llikvalues, nrow = length(gamma), ncol = length(beta), byrow = F)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quindi, il grafico `contour` oppure sua variante `filled.contour` si possono ottenere abbastanza facilmente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "contour(gamma, beta, llikvalues,\n",
        "  xlab = expression(gamma), ylab = expression(beta), # Produce le lettere greche nel grafico\n",
        "  levels = seq(from = -140, to = -120, by = 2),\n",
        "  main = \"Log-verosimiglianza (Modello Weibull)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "filled.contour(gamma, beta, llikvalues,\n",
        "  xlab = expression(gamma), ylab = expression(beta),\n",
        "  levels = seq(from = -140, to = -120, by = 1),\n",
        "  col = terrain.colors(20), main = \"Log-verosimiglianza (Modello Weibull)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stima di massima verosimiglianza\n",
        "\n",
        "Nel caso Weibull la **funzione punteggio** è un vettore di dimensione due, che conduce alle $$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial}{\\partial \\gamma}\\ell(\\gamma, \\beta) & = \\frac{n}{\\gamma} - n \\log\\beta + \\sum_{i=1}^n\\log{y_i} - \\sum_{i=1}^n\\left(\\frac{y_i}{\\beta}\\right)^\\gamma \\log\\left(\\frac{y_i}{\\beta}\\right) = 0, \\\\\n",
        "\\frac{\\partial}{\\partial \\beta}\\ell(\\gamma, \\beta) &= - \\frac{n \\gamma}{\\beta} +\\frac{\\gamma}{\\beta^{\\gamma+1}} \\sum_{i=1}^ny_i^\\gamma = 0.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Risolvendo la seconda equazione otteniamo la stima vincolata $\\hat{\\beta}_\\gamma = \\left(\\frac{1}{n}\\sum_{i=1}^n y_i^\\gamma \\right)^{1/\\gamma}$, per ogni $\\gamma$ fissato.\n",
        "\n",
        "::: callout-note\n",
        "#### Problema chiave\n",
        "\n",
        "Sostituendo $\\hat{\\beta}_\\gamma$ nella prima equazione otteniamo quindi: $$\n",
        "\\frac{n}{\\gamma} + \\sum_{i=1}^n\\log y_i - n \\frac{\\sum_{i=1}^n y_i^\\gamma \\log y_i}{\\sum_{i=1}^n y_i^\\gamma} = 0.\n",
        "$$ Quest'ultima equazione purtroppo **non** ammette una **soluzione analitica**.\n",
        ":::\n",
        "\n",
        "## Stima di massima verosimiglianza\n",
        "\n",
        "Per trovare la **stima** di **massima verosimiglianza** dobbiamo quindi procedere per via numerica.\n",
        "\n",
        "Sebbene esistano in **R** dei comandi che calcolano gli zeri di una funzione (`uniroot`), per identificare il massimo è numericamente più stabile far uso di funzioni dedicate.\n",
        "\n",
        "Oltretutto, questo ci evita di dover fare dei conti analitici, come la derivata prima.\n",
        "\n",
        "Anche in questo caso bivariato, si è specificato il **limite inferiore** del dominio della funzione, ovvero $\\gamma, \\beta > 0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_weibull <- nlminb(start = c(1, 1), function(par) -loglik(par, y), lower = c(1e-7, 1e-7))\n",
        "fit_weibull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot(ecdf(y))\n",
        "curve(pexp(x, fit_exp$par), col = \"red\", add = TRUE)\n",
        "curve(pweibull(x, fit_weibull$par[1], fit_weibull$par[2]), col = \"blue\", add = TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ir",
      "language": "R",
      "display_name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}